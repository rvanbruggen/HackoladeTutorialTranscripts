
# Tutorial 1 - Beginner - What is a data model?

[URL of video: Tutorial 1 - Beginner part 1 - What is a data model?](https://community.hackolade.com/slides/slide/part-1-what-is-a-data-model-3?fullscreen=1)
[Original article over here](https://hackolade.com/help/Whatisadatamodel.html)

## Transcript of the video

Hello everyone, my name is Rik and I welcome you to this new video published by Hackolade, which is going to be a series of videos that compose a tutorial to the Hackolade Studio and all the tools that you can use within that. Today is the first part of that tutorial, and we're going to start at the basics. We're going to start at the start, which is you know, what do we mean, what is it when we talk about a **data model**.

Well, very simply put you know this is actually just a representation of how data is organized. When you store data in some kind of a system somewhere you're going to have to figure out how to sort it, how to organize it and how all of the different concepts, all of the different elements in that data structure are going to relate to each other. So in many ways what you see in a data model is just an explicit representation of the structure of your data.

Why do we do this? Well there's lots of different reasons why you could do that, but at the end of the day it's usually because we want to support the development of our IT systems, and make sure that they're always built and managed and governed in the right way. That means that we need to think about consistency, that we need to think about compatibility and of course that we need to think about the quality of the data that we put into those systems. So when we talk about a data model what we really are talking about is the description of a business or a business application. We're talking about how we're going to build that application, what the blueprint of that application is going to look like. This is useful, as I said earlier, not just because you know you can go back to that afterwards but also during the process, during the development of your systems it's going to help you evaluate your options. There's always more than one way to do something. There's always more than one way to design an application and what the data model is going to help you with is to plan ahead. We really want to think things through before we hand it to the developer and get the system built because we know that if we don't do that, it's going to lead to a lot of work afterwards. It's going to lead to a lot of rework afterwards. 

Something that you will always find in a data model is something we call an **ER diagram**: an _entity relationship diagram_. This is something that many of us have encountered during our computer science studies right but it's a very simple way of understanding those systems. They're just a graphical representation of the entities: the things, the persons, the artifacts, the things that we work with in a system, their characteristics, their attributes (eg. their height, their age, their weight - you know everything that you want to say about a particular entity) and how these different entities are related to each other. A customer places an order - the order is paid with a payment type - you know those types of things are related to each other. So therefore in an entity relationship diagram we're going to represent this visually. It's a way of representing that information and that is something different from what we call a **schema**. A schema is something very different: you know whatever technology that you're going to be using, the schema is actually something very technical. It's something that is not intended for humans to consume. It's actually intended for machines or software programs to consume. Schemas are _machine readable contracts_ between systems right, so the graphical representations you know what we looked at over here - this is for humans, this is for our stakeholders, our individuals that are going to be working with those systems. This thing over here, the schema, you know - you see it on the right hand side here - is what we're going to be using in our systems. This is what the machines, what the software is going to be looking at in order to understand how is this data structured, how is it modeled and obviously there's a link between the graphical representation and the machine-readable schema. Obviously this one is related to the other models, these graphical representations can actually generate these contracts, these schemas - so there's a clear link between the two and obviously you will also see that in your tooling, in things like the Hackolade Studio. But we'll get back to that a little bit later, and for now let's talk about how you start a data model? How do you begin data modeling? 

Well a very normal way to do this, a very common way to do this is to think about your functional requirements. What is this piece of software that we're building, this system that we're building -  what is it going to do? What are the services that it will provide? This is something that you'll ask your end users. What do you want? You will ask them the specifications, the requirements that they want in a functional way for the system to provide. Obviously that's a difficult thing to do, that's a difficult question to ask and therefore this activity, this data modeling activity is usually something that you do **iteratively**. You do it not just once, you actually do it time and time again. You can't really model everything in one go. That would be terribly difficult! If you ask any kind of a stakeholder to describe everything they want it will just lead to chaos. It will become a **big hairball of requirements**, so therefore what people recommend, what we recommend is that you start with the _core concepts_, the core purpose of the application, and then dive into this afterwards. You validate this afterwards as well: you check back in with all the application stakeholders to verify if you've understood them correctly if these functional requirements actually match what they really need. But what a data modeler will always be looking for is a couple of essential components. They will be looking for the key entities in a particular system (what are the things that you're working with, what are the things that you are going to treat, that you're going to modify, that you're going to process in this application), the attributes of those entities (how do you describe them what are their traits and of course how they link to one another, the relationships between those entities). A very common way to do this is to look at sentences that describe this system, and then we can just look at what the nouns are in this sentence. Usually these nouns relate to entities. Can we look at the verbs in those sentences because very often they indicate some kind of an attribute of a of an entity. Or they might say something about how it relates to another entity. 

So if you look at this example here: a customer with a first name and a last name owns a vehicle. The vehicle was registered on a given date and is manufactured by a make. If we pull this apart and we look at all the nouns and the verbs and the these adjectives as well that we have in this sentence and you will very quickly see that there's a couple of entities we're talking about:
* a customer
* a vehicle, and 
* a make right 
And we have got a couple of relationships between those: 
* the customer owns the vehicle 
* the make manufactures the vehicle
We also know a couple of attributes 
* we know the first name
* and the last name
of the customer and then we also know that there's 
* a registration date as an attribute of a vehicle 

So we've got a little bit of a start here. We've got to start we've got the components of our data model. Traditionally when you look at Relational Database Management Systems, you would associate an entity (a customer, a vehicle, or a make) with a physical table. A table in your relational database management system. Your attributes would be columns in those tables. Every instance of an entity would become a row in that table, and then we've got these foreign key relationships that link everything together, that say something about the customer that owns the vehicle, the make that manufactures the vehicle. You know we will do this using foreign key relationships. It's a little bit different though in what we call **nosql databases**, because in those types of databases we use a concept called **denormalization**.

Looking back to that: it does have a number of consequences. There's a couple more things that I would like to explain here about our data modeling activity. 

The thing is there's multiple levels at which you can do this. Traditionally, there have been three there have been three levels: 
1. the **conceptual** level, where we have a high level representation of the data and we talk about entity names, their relationships and some of their attributes, but nothing more detailed than that
2. then we have the **logical** data model which provides more detail about the relationships between the entities, the _primary keys_ the _foreign keys_ - so a little bit more detail 
3. and then last but not least we have the **physical** data model. It really specifies how the data is going to be stored in a particular database of a particular type of a particular brand,  manufacturer, any kind of other storage system that you're going to be using. 

This is how we did it traditionally and in data modeling we always use something called normalization. Normalization is something that you know there's a lot of mathematical foundations to this, but it's basically going to help you structure the data in such a way that you don't have redundancy. You store the data only once, and you ensure consistency. You always know that the data is consistent. They're always structured in the right way. In a normalized database, like for example a relational database management system (something like a SQL server, or a PostgreSQL, or MySQL or Oracle or those types of things), this is how we're going to structure the data. We're going to structure the data in tables with rows and columns, and of course because of this role of normalization, we would try to do that quite early on in the process. Once we've got the concepts defined, the conceptual data model defined, the __Logical data model will usually already have normalization__ as part of it, which makes it more complicated with newer types of data databases and data structures we know and love in the NoSQL world. NoSQL leverages denormalization in function of its access patterns, so we're really looking at a different way of doing data modeling with the advent of these new technologies, these new NoSQL technologies, and that's why there's a really interesting synergy and match between the data modeling practices that we know and love, and the data storage architecture called *polyglot persistence*. 

Polyglot persistence is going to basically tell you that there's no necessity to store all your data in one place. You don't necessarily need to have one database for one application - you could just as well have many different databases. Maybe not many, but at least a few different databases that are the back end for one application, and also you know these databases could be of different types. They could be optimized for different types of workloads and queries. So one application might store its transactional data in a relational database, would store its social network information in the graph database, and would store unstructured data or less structured data in a document database. So the advantage of doing that of using this polyglot persistence architecture obviously is the fact that you can have the best database for every type of data. It will be so much more performant, scalable and flexible because the database is optimized for its particular task. It can really simplify things in many ways as well. Having said that _there's no such thing as a free lunch_! There are disadvantages. It introduces complexity in the applications data storage layer, and the complexity is obviously in the fact that you have all of these different different backends, and you need to keep them in sync. You need to have related data in every one of those systems, so it really requires careful planning and coordination. It's not a free lunch when we do that.

However, and more and more people seem to be implementing these types of architectures, you know we do need to think about how we structure the data in each and every one of those backends. Which is why we think that there's a need for what we call **polyglot data models**. We need more than just the individual physical data model, we also need the logical and conceptual data model in some way. But we're going to call this a "polyglot" data model, that basically provides the umbrella architecture for all of the different systems. Polyglot data models, the way we define them, allow for denormalization if you want. Given the specific access pattern that you're going to use, this particular backend, that backend database form, it will also allow you to derive different types of physical data models and schemas depending on the technology choice that you have made for that part of your application domain, so it will allow you to do that for a variety of different "Target" technologies. 

There's also a very clear link with something we call **domain driven design**. It's an industry term, domain driven design, and it is a way to structure your applications. We can apply the patterns of domain driven design to data modeling. Why? Because there's a need to constrain the environment a little bit. 

We really want you to make sure that your data models your polyglot data models don't become you know these huge structures that no one understands anymore. We want people to focus their attention on their specific domain, on their core. Break down complex problems into smaller ones, and use the data modeling artifacts as a communication tool so that you can basically involve your stakeholders and talk to them and make sure that they understand that they're all aligned on the language that you use inside a new application development effort. We want to keep things together that belong together and we want to reach that shared understanding. The collaboration between the the domain experts and the technical staff and the developers. No need to emphasize this more but it should be iterative, it should be evolutionary.

We continuously refine our requirements so that rather than working with the top structure, where we used to have these three clear layers of conceptual, logical and physical data models - kind of aligned with our different waterfall development methodologies -, now we get into a domain driven approach to software development and to data modeling as well where the domain driven data model will allow you to have a polyglot data architecture and a polyglot data model which can then drive the target specific data models underneath it. 

This is the end of the part one of our tutorial. I hope this was useful there's a lot of reading material on our website and in some of our other videos. I hope this was useful and I look forward to seeing you in part two very very soon. Have a wonderful day.